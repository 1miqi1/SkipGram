{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_ZyGH3YPiiB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import nltk\n",
        "import random\n",
        "import requests\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from typing import Union, List, Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Download and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUTV1gPUQ42R",
        "outputId": "dfc89b12-734f-469e-c297-e10b68be4196"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.gutenberg.org/files/84/84-0.txt\"\n",
        "text_from_url = requests.get(url).text\n",
        "\n",
        "def clean_gutenberg(text: str) -> str:\n",
        "    start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK 84 ***\"\n",
        "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK 84 ***\"\n",
        "    start = text.find(start_marker)\n",
        "    end = text.find(end_marker)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    if start != -1 and end != -1 and start < end:\n",
        "        return text[start + len(start_marker) : end].lower().split()\n",
        "    else:\n",
        "        print(\"Warning: Gutenberg markers not found or in wrong order. Using full text.\")\n",
        "        return text.lower().split()\n",
        "\n",
        "\n",
        "text = clean_gutenberg(text_from_url)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYE4J1qfUd7R"
      },
      "source": [
        "# Data preparation:\n",
        "\n",
        "*   Tokenizer class\n",
        "*   Test data prep\n",
        "*   Sigmoid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8KPx-KWRqrB"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    \"\"\"\n",
        "    Simple tokenizer that maps words to integer tokens and vice versa,\n",
        "    and computes global word frequencies for subsampling and negative sampling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, text: list[str]) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            text: Corpus as a single string.\n",
        "        \"\"\"\n",
        "        self.token_to_id = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "        self.id_to_token = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
        "\n",
        "        # Lowercase and split text\n",
        "        n = 2  # start from 2 because 0 and 1 are reserved\n",
        "\n",
        "        for word in text:\n",
        "            if word not in self.token_to_id:\n",
        "                self.token_to_id[word] = n\n",
        "                self.id_to_token[n] = word\n",
        "                n += 1\n",
        "\n",
        "        self.vocab_size = n\n",
        "        self.vocab = self.token_to_id.keys()\n",
        "        self.tokens = self.tokenize(text)\n",
        "\n",
        "        # Compute global frequencies\n",
        "        self.frequencies = np.zeros(self.vocab_size, dtype=np.float32)\n",
        "        np.add.at(self.frequencies, self.tokens, 1)\n",
        "        self.frequencies /= np.sum(self.frequencies)\n",
        "\n",
        "    def tokenize(self, words: Union[str, List[str]]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Convert word(s) to integer token(s). Unknown words -> <UNK>.\n",
        "        \"\"\"\n",
        "        single = isinstance(words, str)\n",
        "\n",
        "        if single:\n",
        "            words = [words]\n",
        "\n",
        "        tokens = np.array(\n",
        "            [self.token_to_id.get(word, 1) for word in words],\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        return tokens[0] if single else tokens\n",
        "\n",
        "    def detokenize(self, tokens: Union[int, List[int]]) -> Union[str, List[Optional[str]]]:\n",
        "        \"\"\"\n",
        "        Convert token(s) back to word(s).\n",
        "        \"\"\"\n",
        "        # Check if the input is a single integer or numpy integer\n",
        "        single = isinstance(tokens, int) or isinstance(tokens, np.integer)\n",
        "\n",
        "        if single:\n",
        "            tokens = [tokens]\n",
        "\n",
        "        # Explicitly cast each token to a Python int before dictionary lookup\n",
        "        words = [self.id_to_token.get(int(token), \"<UNK>\") for token in tokens]\n",
        "\n",
        "        return words[0] if single else words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5B69SI3Af5l"
      },
      "source": [
        "# Skip-gram Dataset: Core Optimizations\n",
        "\n",
        "The efficiency of Word2Vec relies on three primary data-processing techniques: **Subsampling**, **Dynamic Windows**, and **Negative Sampling Tables**.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Subsampling of Frequent Words\n",
        "To prevent frequent words (stop words) from over-representing in the training data, we drop tokens based on their frequency $f(w_i)$ and a threshold $t$.\n",
        "\n",
        "**Drop Probability Formula:**\n",
        "$$P(\\text{drop } w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}$$\n",
        "\n",
        "* **Threshold $t$:** Usually set around $10^{-5}$.\n",
        "* **Result:** Rare words are kept, while words like \"the\" are discarded frequently, allowing the model to focus on meaningful semantic relationships.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Dynamic Context Window\n",
        "Instead of a fixed window size, we sample a random window for every center word encounter.\n",
        "\n",
        "**Logic:**\n",
        "For each center word $w_c$, pick a random integer $R$:\n",
        "$$1 \\le R \\le \\text{max_window_size}$$\n",
        "\n",
        "* **Impact:** This ensures that words immediately adjacent to the center word are sampled more often than words at the periphery, naturally weighting local context higher.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Negative Sampling Table (Unigram Distribution)\n",
        "To avoid the $O(|V|)$ cost of sampling from the entire vocabulary during the training loop, we pre-calculate a large lookup table.\n",
        "\n",
        "**Sampling Probability:**\n",
        "$$P(w) = \\frac{f(w)^{0.75}}{\\sum_{v \\in V} f(v)^{0.75}}$$\n",
        "\n",
        "\n",
        "\n",
        "### Why the $0.75$ Power?\n",
        "* If $P(\\text{\"the\"}) = 0.9$ and $P(\\text{\"zebra\"}) = 0.0001$:\n",
        "* After the $0.75$ power, the gap shrinks.\n",
        "* **Result:** The model samples rare words slightly more often than their raw frequency would suggest, making the negative classification task more robust.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Implementation Summary Table\n",
        "\n",
        "| Technique | Goal | Key Hyperparameter |\n",
        "| :--- | :--- | :--- |\n",
        "| **Subsampling** | Reduce redundancy of stop words | `subsample_threshold` ($t$) |\n",
        "| **Dynamic Window** | Weight local context higher | `max_window_size` |\n",
        "| **Negative Sampling** | Turn Softmax into Binary Class. | `num_negatives` ($K$) |\n",
        "| **Lookup Table** | $O(1)$ sampling efficiency | `neg_table_size` |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzAChyzzMvkv"
      },
      "outputs": [],
      "source": [
        "class SkipGramDataset:\n",
        "    \"\"\"\n",
        "    Skip-gram dataset with subsampling, dynamic context window, and negative sampling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokens: np.ndarray,\n",
        "        tokenizer: Tokenizer,\n",
        "        subsample_threshold: float = 1e-5,\n",
        "        max_window_size: int = 5,\n",
        "        num_negatives: int = 5,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tokens: Tokenized corpus as numpy array.\n",
        "            tokenizer: Tokenizer object with frequencies.\n",
        "            subsample_threshold: Threshold t for subsampling frequent words.\n",
        "            max_window_size: Maximum size of dynamic context window.\n",
        "            num_negatives: Number of negative samples per positive pair.\n",
        "        \"\"\"\n",
        "        self.tokens = tokens\n",
        "        self.tokenizer = tokenizer\n",
        "        self.subsample_threshold = subsample_threshold\n",
        "        self.max_window_size = max_window_size\n",
        "        self.num_negatives = num_negatives\n",
        "        self.vocab_size = tokenizer.vocab_size\n",
        "\n",
        "        # Subsampling drop probabilities (use global frequencies)\n",
        "        freqs = np.clip(tokenizer.frequencies, 1e-10, None)\n",
        "        self.drop_probs = 1 - np.sqrt(subsample_threshold / freqs)\n",
        "\n",
        "        # Precompute negative sampling table (unigram^0.75)\n",
        "        prob = tokenizer.frequencies ** 0.75\n",
        "        prob /= prob.sum()\n",
        "        self.neg_table = prob\n",
        "\n",
        "        # Step 1: subsample tokens\n",
        "        self.filtered_tokens = [\n",
        "            token for token in tokens if random.random() >= self.drop_probs[token]\n",
        "        ]\n",
        "\n",
        "        # Step 2: generate (center, context, negatives)\n",
        "        (\n",
        "            self.centers,\n",
        "            self.contexts,\n",
        "            self.negatives,\n",
        "        ) = self._generate_examples()\n",
        "\n",
        "    def _generate_examples(self) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Generate (center, context, negatives) for Skip-gram training.\n",
        "        \"\"\"\n",
        "        centers = []\n",
        "        contexts = []\n",
        "        negatives = []\n",
        "        n = len(self.filtered_tokens)\n",
        "\n",
        "        for i, center in enumerate(self.filtered_tokens):\n",
        "            window_size = np.random.randint(1, self.max_window_size + 1)\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(n, i + window_size + 1)\n",
        "            for j in range(start, end):\n",
        "                if j != i:\n",
        "                    centers.append(center)\n",
        "                    contexts.append(self.filtered_tokens[j])\n",
        "\n",
        "                    # Normal distribution\n",
        "                    neg_sample = np.random.choice(\n",
        "                        self.vocab_size, size=self.num_negatives, replace=False, p = self.neg_table\n",
        "                    )\n",
        "                    negatives.append(neg_sample)\n",
        "\n",
        "        return (\n",
        "            np.array(centers, dtype=np.int32),\n",
        "            np.array(contexts, dtype=np.int32),\n",
        "            np.array(negatives, dtype=np.int32),\n",
        "        )\n",
        "\n",
        "    def get_training_batches(\n",
        "        self, batch_size: int\n",
        "    ) -> list[tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
        "        \"\"\"\n",
        "        Shuffle the dataset and split into batches.\n",
        "\n",
        "        Returns:\n",
        "            List of tuples: (batch_centers, batch_contexts, batch_negatives)\n",
        "        \"\"\"\n",
        "        indices = np.arange(len(self.centers))\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        centers_shuffled = self.centers[indices]\n",
        "        contexts_shuffled = self.contexts[indices]\n",
        "        negatives_shuffled = self.negatives[indices]\n",
        "\n",
        "        batches = []\n",
        "        N = len(centers_shuffled)\n",
        "\n",
        "        for i in range(0, N, batch_size):\n",
        "            batch_centers = centers_shuffled[i : i + batch_size]\n",
        "            batch_contexts = contexts_shuffled[i : i + batch_size]\n",
        "            batch_negatives = negatives_shuffled[i : i + batch_size]\n",
        "            batches.append((batch_centers, batch_contexts, batch_negatives))\n",
        "\n",
        "        return batches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grIvm9DHayoI"
      },
      "source": [
        "# Word2Vec implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bnbyepmBGx2"
      },
      "source": [
        "### Embedding layer implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUKh2pJfW1Tk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import Optional, Union, Sequence\n",
        "\n",
        "\n",
        "class EmbeddingLayer:\n",
        "    \"\"\"\n",
        "    An embedding layer with support for sparse gradient accumulation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, embedding_size: int) -> None:\n",
        "        # Small random init (DO NOT normalize)\n",
        "        self.weights = np.random.randn(vocab_size, embedding_size) * 0.01\n",
        "        self.grad_accumulator = None\n",
        "        self.indices = None\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "    def merge_duplicate_indices(self) -> None:\n",
        "        \"\"\"Combine gradients for repeated indices.\"\"\"\n",
        "        if self.indices is None:\n",
        "            return\n",
        "\n",
        "        unique_indices, inverse = np.unique(self.indices, return_inverse=True)\n",
        "        out = np.zeros((len(unique_indices), self.embedding_size), dtype=self.weights.dtype)\n",
        "        np.add.at(out, inverse, self.grad_accumulator)\n",
        "\n",
        "        self.indices = unique_indices\n",
        "        self.grad_accumulator = out\n",
        "\n",
        "    def accumulate_gradients(self, grad: np.ndarray, indices: np.ndarray) -> None:\n",
        "        \"\"\"Accumulate gradients for given indices.\"\"\"\n",
        "        grad = np.asarray(grad, dtype=self.weights.dtype).reshape(-1, self.embedding_size)\n",
        "        indices = np.asarray(indices, dtype=np.int64).reshape(-1)\n",
        "\n",
        "        if self.grad_accumulator is None:\n",
        "            self.grad_accumulator = grad\n",
        "            self.indices = indices\n",
        "        else:\n",
        "            self.grad_accumulator = np.concatenate([self.grad_accumulator, grad], axis=0)\n",
        "            self.indices = np.concatenate([self.indices, indices], axis=0)\n",
        "\n",
        "        self.merge_duplicate_indices()\n",
        "\n",
        "    def apply_gradients(self, learning_rate: float = 1.0) -> None:\n",
        "        \"\"\"Apply accumulated gradients using gradient descent.\"\"\"\n",
        "        if self.grad_accumulator is None:\n",
        "            return\n",
        "\n",
        "        # 1. Gradient clipping (prevents explosion)\n",
        "        np.clip(self.grad_accumulator, -5.0, 5.0, out=self.grad_accumulator)\n",
        "\n",
        "        # 2. Gradient DESCENT (minus sign!)\n",
        "        np.add.at(\n",
        "            self.weights,\n",
        "            self.indices,\n",
        "            -learning_rate * self.grad_accumulator\n",
        "        )\n",
        "\n",
        "        self.zero_grad()\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        \"\"\"Clear accumulated gradients.\"\"\"\n",
        "        self.grad_accumulator = None\n",
        "        self.indices = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK9foleeWkCv"
      },
      "source": [
        "# Skip-gram with Negative Sampling (SGNS)\n",
        "\n",
        "The **Skip-gram model** learns word embeddings by using a **center word** to predict its **context words**.\n",
        "\n",
        "### Key Notation\n",
        "* $v_w$: **Input embedding** (when word $w$ is the center word).\n",
        "* $u_w$: **Output embedding** (when word $w$ is the context word).\n",
        "\n",
        "---\n",
        "\n",
        "## 1. The Objective Function\n",
        "\n",
        "For a positive center-context pair $(w_c, w_o)$ and $K$ negative samples $w_i^-$:\n",
        "\n",
        "$$\\mathcal{L} = \\log \\sigma(u_{w_o}^\\top v_{w_c}) + \\sum_{i=1}^K \\log \\sigma(-u_{w_i^-}^\\top v_{w_c})$$\n",
        "\n",
        "Where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid function.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Gradient Derivations\n",
        "\n",
        "To update the vectors using SGD, we use the following gradients:\n",
        "\n",
        "### Gradient w.r.t. Center Vector ($v_{w_c}$)\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial v_{w_c}} = [1 - \\sigma(u_{w_o}^\\top v_{w_c})]u_{w_o} - \\sum_{i=1}^K [1 - \\sigma(-u_{w_i^-}^\\top v_{w_c})]u_{w_i^-}$$\n",
        "\n",
        "### Gradient w.r.t. Positive Context Vector ($u_{w_o}$)\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial u_{w_o}} = [1 - \\sigma(u_{w_o}^\\top v_{w_c})]v_{w_c}$$\n",
        "\n",
        "### Gradient w.r.t. Negative Context Vector ($u_{w_i^-}$)\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial u_{w_i^-}} = -[1 - \\sigma(-u_{w_i^-}^\\top v_{w_c})]v_{w_c}$$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmVwLYK0I8Dd"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEg86Ew71Lxv"
      },
      "outputs": [],
      "source": [
        "class SkipGram:\n",
        "    \"\"\"\n",
        "    Skip-gram model for learning word embeddings using negative sampling.\n",
        "\n",
        "    Attributes:\n",
        "        vocab_size (int): Size of the vocabulary.\n",
        "        embedding_size (int): Dimensionality of the embeddings.\n",
        "        input_embedding (EmbeddingLayer): Embedding layer for center words.\n",
        "        output_embedding (EmbeddingLayer): Embedding layer for context/negative words.\n",
        "        calculate_gradient (bool): Whether to compute gradients during forward pass.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, embedding_size: int):\n",
        "        \"\"\"\n",
        "        Initialize the SkipGram model with input and output embeddings.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Number of unique tokens in the vocabulary.\n",
        "            embedding_size (int): Dimensionality of the embedding vectors.\n",
        "        \"\"\"\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.input_embedding = EmbeddingLayer(vocab_size=vocab_size, embedding_size=embedding_size)\n",
        "        self.output_embedding = EmbeddingLayer(vocab_size=vocab_size, embedding_size=embedding_size)\n",
        "        self.calculate_gradient = True\n",
        "\n",
        "    def forward(self, centers, contexts, negatives):\n",
        "        \"\"\"\n",
        "        Compute the forward pass of the Skip-gram model with negative sampling.\n",
        "\n",
        "        Args:\n",
        "            centers (np.ndarray): Array of center word indices, shape (batch_size,).\n",
        "            contexts (np.ndarray): Array of positive context word indices, shape (batch_size,).\n",
        "            negatives (np.ndarray): Array of negative sample indices, shape (batch_size, neg_size).\n",
        "\n",
        "        Returns:\n",
        "            float: The average negative log-likelihood loss for the batch.\n",
        "\n",
        "        Notes:\n",
        "            If `calculate_gradient` is True, this function also computes and accumulates\n",
        "            gradients for both input and output embeddings.\n",
        "        \"\"\"\n",
        "        batch_size = centers.shape[0]\n",
        "        neg_size = negatives.shape[1]\n",
        "\n",
        "        # Look up embeddings\n",
        "        v_c = self.input_embedding.weights[centers]        # Center embeddings (batch_size, embedding_size)\n",
        "        u_w = self.output_embedding.weights[contexts]     # Positive context embeddings (batch_size, embedding_size)\n",
        "        u_n = self.output_embedding.weights[negatives]    # Negative sample embeddings (batch_size, neg_size, embedding_size)\n",
        "\n",
        "        # Positive score: dot product of center and context embeddings\n",
        "        pos_dot = np.sum(v_c * u_w, axis=1, keepdims=True)\n",
        "        pos_prob = sigmoid(pos_dot)\n",
        "\n",
        "        # Negative score: dot product of center and negative embeddings\n",
        "        neg_dot = np.sum(u_n * v_c[:, np.newaxis, :], axis=2)\n",
        "        neg_prob = sigmoid(-neg_dot)\n",
        "\n",
        "        # Compute loss: Negative Log-Likelihood\n",
        "        loss = -np.mean(np.log(pos_prob + 1e-9) + np.sum(np.log(neg_prob + 1e-9), axis=1, keepdims=True))\n",
        "\n",
        "        if self.calculate_gradient:\n",
        "            # Gradients w.r.t positive and negative dot products\n",
        "            grad_pos = pos_prob - 1\n",
        "            grad_neg = 1 - neg_prob\n",
        "\n",
        "            # Gradients for embeddings\n",
        "            v_c_grad = (grad_pos * u_w) + np.sum(grad_neg[:, :, np.newaxis] * u_n, axis=1)\n",
        "            u_w_grad = grad_pos * v_c\n",
        "            u_n_grad = grad_neg[:, :, np.newaxis] * v_c[:, np.newaxis, :]\n",
        "\n",
        "            # Accumulate gradients in embedding layers\n",
        "            self.input_embedding.accumulate_gradients(v_c_grad / batch_size, centers)\n",
        "            self.output_embedding.accumulate_gradients(u_w_grad / batch_size, contexts)\n",
        "            self.output_embedding.accumulate_gradients(u_n_grad / batch_size, negatives)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self, learning_rate: float = 1.0):\n",
        "        \"\"\"\n",
        "        Apply accumulated gradients to update embedding weights.\n",
        "\n",
        "        Args:\n",
        "            learning_rate (float, optional): Learning rate for gradient updates. Defaults to 1.0.\n",
        "        \"\"\"\n",
        "        self.input_embedding.apply_gradients(learning_rate=learning_rate)\n",
        "        self.output_embedding.apply_gradients(learning_rate=learning_rate)\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.input_embedding.zero_grad()\n",
        "        self.output_embedding.zero_grad()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zOmLDh47gaA"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKrhzaF9JXat"
      },
      "outputs": [],
      "source": [
        "def train_epoch(epoch, dataset, learning_rate, model, verbose):\n",
        "    \"\"\"\n",
        "    Train the Skip-gram model for one epoch on the provided dataset.\n",
        "\n",
        "    Args:\n",
        "        epoch (int): Current epoch index (0-based).\n",
        "        dataset (tuple): A tuple containing three lists/arrays:\n",
        "            - batches_centres: List/array of center word batches.\n",
        "            - batches_contexts: List/array of positive context word batches.\n",
        "            - batches_negatives: List/array of negative sample batches.\n",
        "        learning_rate (float): Learning rate for updating embeddings.\n",
        "        model (SkipGram): The Skip-gram model instance to train.\n",
        "        verbose (bool): Whether to print progress and loss information.\n",
        "\n",
        "    Returns:\n",
        "        float: The average loss over all batches for this epoch.\n",
        "\n",
        "    \"\"\"\n",
        "    total_loss = 0\n",
        "    num_batches = len(dataset)\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    for batch in dataset:\n",
        "        centre, context, negatives = batch\n",
        "        loss = model.forward(centre, context, negatives)\n",
        "        total_loss += loss\n",
        "        model.backward(learning_rate=learning_rate)\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Epoch: {epoch + 1}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def training_loop(num_epochs, dataset, learning_rate, model):\n",
        "    \"\"\"\n",
        "    Train the Skip-gram model over multiple epochs.\n",
        "\n",
        "    Args:\n",
        "        num_epochs (int): Number of epochs to train.\n",
        "        dataset (tuple): Dataset tuple containing batches of centers, contexts, and negatives.\n",
        "        learning_rate (float): Learning rate for embedding updates.\n",
        "        model (SkipGram): The Skip-gram model instance to train.\n",
        "\n",
        "    Notes:\n",
        "        - Enables gradient calculation in the model.\n",
        "        - Calls `train_epoch` for each epoch.\n",
        "        - Prints progress every 5 epochs and the first epoch.\n",
        "        - Does not return a value; training updates are applied directly to the model.\n",
        "    \"\"\"\n",
        "    model.calculate_gradient = True\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        verbose = (epoch + 1) % 5 == 0 or epoch == 0\n",
        "\n",
        "        train_epoch(\n",
        "            epoch=epoch,\n",
        "            dataset=dataset,\n",
        "            learning_rate=learning_rate,\n",
        "            model=model,\n",
        "            verbose=verbose\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZIOURkAEIyx"
      },
      "outputs": [],
      "source": [
        "sample_text = text[:10000]\n",
        "tokenizer = Tokenizer(sample_text)\n",
        "tokens = tokenizer.tokenize(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g71SshUddhbD"
      },
      "outputs": [],
      "source": [
        "skip_gram_dataset = SkipGramDataset(tokens = tokens, tokenizer=tokenizer, num_negatives=20, max_window_size=10)\n",
        "dataset = skip_gram_dataset.get_training_batches(batch_size=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eHVt39mR7cU",
        "outputId": "8b9850c5-9aec-44ef-aff6-601d38708330"
      },
      "outputs": [],
      "source": [
        "vocab_size = tokenizer.vocab_size\n",
        "embedding_size = 50\n",
        "\n",
        "skpGram = SkipGram(vocab_size=vocab_size, embedding_size= embedding_size)\n",
        "\n",
        "training_loop(\n",
        "    num_epochs=50,\n",
        "    dataset=dataset,\n",
        "    learning_rate=0.1,\n",
        "    model=skpGram\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyJtKqNfcEzy"
      },
      "outputs": [],
      "source": [
        "def get_similar_words(word, embedding_matrix, tokenizer, k=5):\n",
        "    \"\"\"\n",
        "    word_idx: The index of the word you want to test\n",
        "    embedding_matrix: Your trained self.input_embedding.weight\n",
        "    vocab_dict: A dictionary mapping {id: \"word\"} to print results\n",
        "    \"\"\"\n",
        "    word_idx = tokenizer.tokenize(word)\n",
        "\n",
        "    # 1. Normalize all vectors to unit length (L2 norm)\n",
        "    # This turns dot product into cosine similarity\n",
        "    norm = np.linalg.norm(embedding_matrix, axis=1, keepdims=True)\n",
        "    normalized_embeds = embedding_matrix / (norm + 1e-9)\n",
        "\n",
        "    # 2. Get the specific vector for our word\n",
        "    query_vector = normalized_embeds[word_idx]\n",
        "\n",
        "    # 3. Calculate dot product with ALL other words\n",
        "    # (embedding_size,) @ (vocab_size, embedding_size).T -> (vocab_size,)\n",
        "    scores = normalized_embeds @ query_vector\n",
        "\n",
        "    # 4. Get indices of top K scores (excluding the word itself at the very top)\n",
        "    # argsort sorts ascending, so we take the last k+1 and reverse them\n",
        "    nearest_idx = np.argsort(scores)[-(k+1):-1][::-1]\n",
        "\n",
        "    print(f\"Words most similar to '{word}':\")\n",
        "    for idx in nearest_idx:\n",
        "        print(f\"  - {tokenizer.detokenize(idx)} (score: {scores[idx]:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu37iVq2YiNk",
        "outputId": "4a00e581-1818-43ab-a5f2-2b8726ad3179"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHJkIPE2ihrC",
        "outputId": "64496dcb-a3be-4740-a75b-aaf164e92e77"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-VjfUyCZmZ7",
        "outputId": "11d1722e-bf3a-4290-9cb0-a8ea7da3626a"
      },
      "outputs": [],
      "source": [
        "get_similar_words('summer', embedding_matrix = skpGram.input_embedding.weights, tokenizer=tokenizer, k = 10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ven",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
