{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Skip-gram Word2Vec from Scratch\n",
        "This notebook implements the **Skip-gram with Negative Sampling (SGNS)** architecture using only NumPy. We cover:\n",
        "1. **Text Preprocessing**: Fetching and cleaning Gutenberg data.\n",
        "2. **Subsampling**: Reducing the influence of frequent stop words.\n",
        "3. **Negative Sampling**: Efficiently training against 'noise' words.\n",
        "4. **Backpropagation**: Manual gradient derivations for embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import nltk\n",
        "import random\n",
        "import requests\n",
        "import re\n",
        "from typing import Union, List, Optional, Tuple, Dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Acquisition & Cleaning\n",
        "We fetch *Frankenstein* from Project Gutenberg and remove the metadata headers/footers to ensure we are only training on the literary prose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_gutenberg(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Cleans Gutenberg ebook text by removing metadata and punctuation.\n",
        "    \n",
        "    Args:\n",
        "        text: Raw string from Gutenberg URL.\n",
        "    Returns:\n",
        "        List of lowercase word tokens.\n",
        "    \"\"\"\n",
        "    start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK 84 ***\"\n",
        "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK 84 ***\"\n",
        "    \n",
        "    start = text.find(start_marker)\n",
        "    end = text.find(end_marker)\n",
        "    \n",
        "    # Regex to keep only alphanumeric characters and spaces\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    if start != -1 and end != -1 and start < end:\n",
        "        return text[start + len(start_marker) : end].lower().split()\n",
        "    else:\n",
        "        print(\"Warning: Markers not found. Using full text.\")\n",
        "        return text.lower().split()\n",
        "\n",
        "url = \"https://www.gutenberg.org/files/84/84-0.txt\"\n",
        "raw_data = requests.get(url).text\n",
        "text_tokens = clean_gutenberg(raw_data)\n",
        "print(f\"Corpus size: {len(text_tokens)} tokens.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. The Tokenizer\n",
        "The Tokenizer maps words to unique integer IDs and calculates the global unigram frequency distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    \"\"\"\n",
        "    Maps words to IDs and maintains frequency distributions for sampling.\n",
        "    \"\"\"\n",
        "    def __init__(self, text: List[str]) -> None:\n",
        "        self.token_to_id = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "        self.id_to_token = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
        "\n",
        "        # Build Vocab\n",
        "        curr_id = 2 \n",
        "        for word in text:\n",
        "            if word not in self.token_to_id:\n",
        "                self.token_to_id[word] = curr_id\n",
        "                self.id_to_token[curr_id] = word\n",
        "                curr_id += 1\n",
        "\n",
        "        self.vocab_size = curr_id\n",
        "        self.tokens = self.tokenize(text)\n",
        "\n",
        "        # Compute frequencies\n",
        "        self.frequencies = np.zeros(self.vocab_size, dtype=np.float32)\n",
        "        np.add.at(self.frequencies, self.tokens, 1)\n",
        "        self.frequencies /= np.sum(self.frequencies)\n",
        "\n",
        "    def tokenize(self, words: Union[str, List[str]]) -> np.ndarray:\n",
        "        single = isinstance(words, str)\n",
        "        if single: words = [words]\n",
        "        \n",
        "        ids = np.array([self.token_to_id.get(w, 1) for w in words], dtype=np.int32)\n",
        "        return ids[0] if single else ids\n",
        "\n",
        "    def detokenize(self, tokens: Union[int, List[int]]) -> Union[str, List[str]]:\n",
        "        single = isinstance(tokens, (int, np.integer))\n",
        "        if single: tokens = [tokens]\n",
        "        \n",
        "        words = [self.id_to_token.get(int(t), \"<UNK>\") for t in tokens]\n",
        "        return words[0] if single else words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset Optimizations\n",
        "To make Word2Vec efficient, we use:\n",
        "\n",
        "### Subsampling\n",
        "Frequent words are dropped with probability:\n",
        "$$P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}$$\n",
        "\n",
        "### Negative Sampling\n",
        "Instead of calculating probabilities for the entire vocabulary, we sample $K$ noise words from a smoothed distribution:\n",
        "$$P_{ns}(w) = \\frac{f(w)^{0.75}}{\\sum_{v \\in V} f(v)^{0.75}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SkipGramDataset:\n",
        "    \"\"\"\n",
        "    Generates (center, context, negative) triplets for training.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, \n",
        "        tokens: np.ndarray, \n",
        "        tokenizer: Tokenizer, \n",
        "        subsample_threshold: float = 1e-5, \n",
        "        max_window_size: int = 5,\n",
        "        num_negatives: int = 5\n",
        "    ) -> None:\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_window_size = max_window_size\n",
        "        self.num_negatives = num_negatives\n",
        "\n",
        "        # Subsampling Logic\n",
        "        freqs = np.clip(tokenizer.frequencies, 1e-10, None)\n",
        "        drop_probs = 1 - np.sqrt(subsample_threshold / freqs)\n",
        "        self.filtered_tokens = [t for t in tokens if random.random() >= drop_probs[t]]\n",
        "\n",
        "        # Negative Sampling Distribution (Smoothed)\n",
        "        prob = tokenizer.frequencies ** 0.75\n",
        "        self.neg_table = prob / prob.sum()\n",
        "\n",
        "        self.centers, self.contexts, self.negatives = self._generate_examples()\n",
        "\n",
        "    def _generate_examples(self):\n",
        "        centers, contexts, negatives = [], [], []\n",
        "        n = len(self.filtered_tokens)\n",
        "\n",
        "        for i, center in enumerate(self.filtered_tokens):\n",
        "            # Dynamic Window\n",
        "            window = np.random.randint(1, self.max_window_size + 1)\n",
        "            start, end = max(0, i - window), min(n, i + window + 1)\n",
        "            \n",
        "            for j in range(start, end):\n",
        "                if i == j: continue\n",
        "                centers.append(center)\n",
        "                contexts.append(self.filtered_tokens[j])\n",
        "                \n",
        "                # Negative Sampling\n",
        "                neg = np.random.choice(self.tokenizer.vocab_size, size=self.num_negatives, p=self.neg_table)\n",
        "                negatives.append(neg)\n",
        "        \n",
        "        return np.array(centers), np.array(contexts), np.array(negatives)\n",
        "\n",
        "    def get_batches(self, batch_size: int):\n",
        "        idx = np.arange(len(self.centers))\n",
        "        np.random.shuffle(idx)\n",
        "        \n",
        "        batches = []\n",
        "        for i in range(0, len(idx), batch_size):\n",
        "            b_idx = idx[i : i + batch_size]\n",
        "            batches.append((self.centers[b_idx], self.contexts[b_idx], self.negatives[b_idx]))\n",
        "        return batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. The Model (SGNS)\n",
        "We maintain two sets of embeddings: $V$ (Input/Center) and $U$ (Output/Context).\n",
        "\n",
        "### Objective Function\n",
        "$$\\mathcal{L} = -\\log \\sigma(u_{context}^\\top v_{center}) - \\sum_{i=1}^K \\log \\sigma(-u_{neg_i}^\\top v_{center})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "    return 1 / (1 + np.exp(-np.clip(x, -15, 15)))\n",
        "\n",
        "class EmbeddingLayer:\n",
        "    \"\"\"Sparse embedding layer with gradient accumulation.\"\"\"\n",
        "    def __init__(self, vocab_size, dim):\n",
        "        self.weights = np.random.randn(vocab_size, dim) * 0.01\n",
        "        self.zero_grad()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.grad_acc = None\n",
        "        self.indices = None\n",
        "\n",
        "    def accumulate(self, grad, indices):\n",
        "        grad = np.asarray(grad).reshape(-1, self.weights.shape[1])\n",
        "        indices = np.asarray(indices).reshape(-1)\n",
        "        \n",
        "        if self.grad_acc is None:\n",
        "            self.grad_acc, self.indices = grad, indices\n",
        "        else:\n",
        "            self.grad_acc = np.concatenate([self.grad_acc, grad])\n",
        "            self.indices = np.concatenate([self.indices, indices])\n",
        "\n",
        "    def apply_grads(self, lr):\n",
        "        if self.grad_acc is None: return\n",
        "        # Sparse update: merge duplicate indices\n",
        "        unique_idx, inv = np.unique(self.indices, return_inverse=True)\n",
        "        merged_grad = np.zeros((len(unique_idx), self.weights.shape[1]))\n",
        "        np.add.at(merged_grad, inv, self.grad_acc)\n",
        "        \n",
        "        np.add.at(self.weights, unique_idx, -lr * merged_grad)\n",
        "        self.zero_grad()\n",
        "\n",
        "class SkipGram:\n",
        "    def __init__(self, vocab_size, dim):\n",
        "        self.in_embed = EmbeddingLayer(vocab_size, dim)\n",
        "        self.out_embed = EmbeddingLayer(vocab_size, dim)\n",
        "\n",
        "    def forward(self, centers, contexts, negatives):\n",
        "        batch_size = len(centers)\n",
        "        \n",
        "        v_c = self.in_embed.weights[centers] # (B, E)\n",
        "        u_w = self.out_embed.weights[contexts] # (B, E)\n",
        "        u_n = self.out_embed.weights[negatives] # (B, K, E)\n",
        "\n",
        "        # Positive score\n",
        "        pos_score = np.sum(v_c * u_w, axis=1, keepdims=True)\n",
        "        pos_sig = sigmoid(pos_score)\n",
        "\n",
        "        # Negative scores\n",
        "        neg_score = np.sum(u_n * v_c[:, np.newaxis, :], axis=2)\n",
        "        neg_sig = sigmoid(-neg_score)\n",
        "\n",
        "        # Loss\n",
        "        loss = -np.mean(np.log(pos_sig + 1e-9) + np.sum(np.log(neg_sig + 1e-9), axis=1, keepdims=True))\n",
        "\n",
        "        # Gradients\n",
        "        grad_pos = pos_sig - 1\n",
        "        grad_neg = 1 - neg_sig\n",
        "\n",
        "        v_c_grad = (grad_pos * u_w) + np.sum(grad_neg[:, :, np.newaxis] * u_n, axis=1)\n",
        "        u_w_grad = grad_pos * v_c\n",
        "        u_n_grad = grad_neg[:, :, np.newaxis] * v_c[:, np.newaxis, :]\n",
        "\n",
        "        self.in_embed.accumulate(v_c_grad / batch_size, centers)\n",
        "        self.out_embed.accumulate(u_w_grad / batch_size, contexts)\n",
        "        self.out_embed.accumulate(u_n_grad / batch_size, negatives)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Loop\n",
        "We iterate through the dataset and update the embeddings via SGD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, dataset, epochs, lr):\n",
        "    for epoch in range(epochs):\n",
        "        batches = dataset.get_batches(batch_size=256)\n",
        "        total_loss = 0\n",
        "        for c, ctx, neg in batches:\n",
        "            loss = model.forward(c, ctx, neg)\n",
        "            total_loss += loss\n",
        "            model.in_embed.apply_grads(lr)\n",
        "            model.out_embed.apply_grads(lr)\n",
        "        \n",
        "        if (epoch+1) % 5 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1} | Avg Loss: {total_loss/len(batches):.4f}\")\n",
        "\n",
        "# Run Training\n",
        "tok = Tokenizer(text_tokens[:20000])\n",
        "ds = SkipGramDataset(tok.tokens, tok, num_negatives=5)\n",
        "sg_model = SkipGram(tok.vocab_size, dim=50)\n",
        "\n",
        "train(sg_model, ds, epochs=50, lr=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inference: Similarity Search\n",
        "Using Cosine Similarity to find words that occupy similar semantic space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_similar(word, model, tok, k=5):\n",
        "    if word not in tok.token_to_id: return f\"{word} not in vocab\"\n",
        "    \n",
        "    weights = model.in_embed.weights\n",
        "    norm = np.linalg.norm(weights, axis=1, keepdims=True)\n",
        "    norm_weights = weights / (norm + 1e-9)\n",
        "    \n",
        "    vec = norm_weights[tok.tokenize(word)]\n",
        "    sims = norm_weights @ vec\n",
        "    \n",
        "    top_idx = np.argsort(sims)[-(k+1):-1][::-1]\n",
        "    print(f\"Similar to '{word}':\")\n",
        "    for i in top_idx:\n",
        "        print(f\" - {tok.detokenize(i)} ({sims[i]:.4f})\")\n",
        "\n",
        "get_similar(\"monster\", sg_model, tok)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
